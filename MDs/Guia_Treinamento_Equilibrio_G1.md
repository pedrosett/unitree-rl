# Guia Did√°tico: Treinamento de Policy de Equil√≠brio para Unitree G1

![Status](https://img.shields.io/badge/Status-Pesquisa%20Conclu%C3%ADda-blue)
![N√≠vel](https://img.shields.io/badge/N%C3%ADvel-Iniciante%20a%20Avan%C3%A7ado-green)
![Tempo](https://img.shields.io/badge/Tempo-30min%20a%204h-orange)

## üéØ **Problema Identificado**

Nosso rob√¥ Unitree G1 est√° **resetando constantemente** (caindo a cada 100-200 steps) porque estamos usando um modelo **sub-treinado** com apenas **10 itera√ß√µes**. √â como tentar ensinar algu√©m a andar de bicicleta em apenas 30 segundos!

## üß† **Integra√ß√£o WASD + Equil√≠brio: Modelo √önico**

### **Por que Um Modelo Neural √önico?**

Baseado em **pesquisas cient√≠ficas recentes** (2024-2025), a abordagem correta √© usar **um √∫nico modelo neural** que aprende m√∫ltiplos comportamentos simultaneamente:

**‚úÖ Evid√™ncias Cient√≠ficas:**
- **Multi-Task Learning (MTL)**: Frameworks que treinam uma √∫nica rede neural para m√∫ltiplas tarefas (walking, turning, standing)
- **Unitree Success Cases**: Valida√ß√£o bem-sucedida em Go1, A1, G1 com pol√≠ticas √∫nicas
- **End-to-End Control**: Pol√≠ticas que executam m√∫ltiplos gaits diretamente de inputs proprioceptivos
- **Biology-Inspired CPG-RL**: Central Pattern Generators + Deep RL em uma √∫nica policy

**‚ùå Por que N√ÉO usar modelos separados:**
- Complexidade de coordena√ß√£o entre m√∫ltiplos modelos
- Transi√ß√µes abruptas entre comportamentos
- Overhead computacional de m√∫ltiplas infer√™ncias
- Perda de contexto entre mudan√ßas de modelo

### **Como Nossa Implementa√ß√£o Funciona**

**Fluxo de Integra√ß√£o WASD ‚Üí Equil√≠brio:**
```
WASD Input ‚Üí [vx, wz] ‚Üí env.commands ‚Üí observations[47] ‚Üí policy ‚Üí joint_actions[12] ‚Üí movimento_equilibrado
```

**Estados Aprendidos Simultaneamente:**
- **STANDING** (`vx=0, wz=0`): Equil√≠brio est√°tico
- **WALKING** (`vx‚â†0, wz=0`): Locomo√ß√£o linear + equil√≠brio
- **TURNING** (`vx=0, wz‚â†0`): Rota√ß√£o no lugar + equil√≠brio  
- **COMBINED** (`vx‚â†0, wz‚â†0`): Movimento complexo + equil√≠brio

**Interface de Comando J√° Implementada:**
```python
# No play.py - j√° funciona!
env.commands[:, 0] = vx_cmd    # Velocidade linear (WASD W/S)
env.commands[:, 1] = 0.0       # Velocidade lateral (zero para human√≥ide)
env.commands[:, 2] = wz_cmd    # Velocidade angular (WASD A/D)
```

**Recompensas Integradas (g1_config.py):**
```python
tracking_lin_vel = 1.0    # Seguir comandos de velocidade ‚Üí WASD responsivo
tracking_ang_vel = 0.5    # Seguir rota√ß√£o ‚Üí WASD turning
alive = 0.15              # Manter equil√≠brio durante movimento
base_height = -10.0       # Penalidade por perder postura
orientation = -1.0        # Penalidade por inclinar
```

**üéØ Resultado: WASD j√° integrado! S√≥ falta treinar at√© converg√™ncia.**

---

## üìö **Se√ß√£o 1: Conceitos Fundamentais (Para Leigos)**

### üß† **1.1 O que √© uma "Policy" em IA?**

**Analogia Simples**: A policy √© como o **"c√©rebro" do rob√¥** que toma decis√µes. 

Imagine que voc√™ est√° dirigindo um carro:
- **Inputs** (sensores): Voc√™ v√™ a estrada, sente o volante, ouve o motor
- **Processamento** (policy): Seu c√©rebro decide "virar √† esquerda", "acelerar", "frear"  
- **Outputs** (a√ß√µes): Suas m√£os e p√©s executam os movimentos

Para o rob√¥ G1:
- **Inputs**: 47 sensores (posi√ß√£o das juntas, orienta√ß√£o, velocidades, etc.)
- **Policy**: Rede neural que processa esses dados
- **Outputs**: 12 comandos para os motores das juntas

### üéÆ **1.2 Reinforcement Learning (Aprendizado por Refor√ßo)**

**Analogia**: √â como **treinar um animal de estima√ß√£o**:

1. **Animal tenta** uma a√ß√£o (rob√¥ tenta um movimento)
2. **Voc√™ avalia** o resultado (sistema d√° uma "nota"/reward)
3. **Animal aprende** a repetir boas a√ß√µes (policy melhora)
4. **Repete milhares de vezes** at√© aprender perfeitamente

**Por que usar RL para rob√≥tica?**
- ‚ùå **Programa√ß√£o manual**: Imposs√≠vel prever todas as situa√ß√µes
- ‚ùå **F√≠sica simulada**: Muito complexa para calcular manualmente  
- ‚úÖ **RL**: O rob√¥ **descobre sozinho** como se equilibrar

### üìñ **1.3 Gloss√°rio de Termos T√©cnicos**

| Termo | Analogia | Explica√ß√£o T√©cnica |
|-------|----------|-------------------|
| **Policy** | "C√©rebro" do rob√¥ | Rede neural que decide a√ß√µes baseada em observa√ß√µes |
| **Checkpoint** | "Save game" | Snapshot do treinamento que pode ser carregado depois |
| **Itera√ß√µes** | "Tentativas de aprendizado" | N√∫mero de vezes que o rob√¥ treinou (nosso: 10, ideal: 3000+) |
| **Episode** | "Uma vida" do rob√¥ | Per√≠odo desde reset at√© pr√≥ximo reset (queda/timeout) |
| **Reward** | "Nota/pontua√ß√£o" | N√∫mero que diz se a√ß√£o foi boa (+) ou ruim (-) |
| **Converg√™ncia** | "Rob√¥ aprendeu" | Quando performance para de melhorar significativamente |
| **Reset** | "Morreu, tenta de novo" | Rob√¥ caiu ou falhou, volta √† posi√ß√£o inicial |

---

## üîç **Se√ß√£o 2: Modelo Pr√©-treinado motion.pt**

### üìÅ **2.1 Resumo da Descoberta**

Encontramos um modelo pr√©-treinado no reposit√≥rio (`deploy/pre_train/g1/motion.pt` - 145KB, formato TorchScript). Como sua compatibilidade e funcionalidade s√£o desconhecidas, **focaremos no treinamento do zero** para ter controle total sobre o processo e garantir que a policy seja otimizada especificamente para controle WASD teleoperado.

---

---

## ‚öñÔ∏è **Se√ß√£o 2: Por que Nosso Modelo Atual Falha?**

### üìä **2.1 O Problema das 10 Itera√ß√µes**

**Analogia**: Imagine aprender a andar de bicicleta em apenas 30 segundos:
- **Itera√ß√£o 1-3**: Caindo constantemente
- **Itera√ß√£o 4-6**: Ainda inst√°vel, mas melhorando
- **Itera√ß√£o 7-10**: Come√ßando a entender equil√≠brio
- **Nosso modelo parou aqui!** ‚ùå

**Para compara√ß√£o:**
- **100 itera√ß√µes**: B√°sico de equil√≠brio
- **1000 itera√ß√µes**: Caminhada est√°vel  
- **3000 itera√ß√µes**: Movimentos suaves
- **5000+ itera√ß√µes**: Performance robusta

### üìà **2.2 Curva T√≠pica de Aprendizado RL**

```
Reward ‚îÇ
       ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Converg√™ncia (3000+ iter)
   Alto‚îÇ    ‚ï±
       ‚îÇ   ‚ï±
       ‚îÇ  ‚ï±
       ‚îÇ ‚ï±
  Baixo‚îÇ‚ï± ‚Üê Nossos 10 iter
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Itera√ß√µes
       0   500  1000  2000  5000
```

**Fases do Aprendizado:**
1. **0-100**: Explora√ß√£o aleat√≥ria (caos total)
2. **100-500**: Descobrindo padr√µes b√°sicos
3. **500-1500**: Desenvolvendo estrat√©gias
4. **1500-3000**: Refinando movimentos
5. **3000+**: Policy madura e est√°vel

### üéØ **2.3 Sistema de Recompensas (Rewards) do G1**

O rob√¥ "aprende" atrav√©s de um **sistema de notas**:

#### **üèÜ Recompensas Positivas (Rob√¥ ganha pontos)**
```python
alive = +0.15           # Por estar "vivo" (n√£o resetou)
contact = +0.18         # Por p√©s tocarem no ch√£o corretamente
```

#### **‚ö†Ô∏è Penalidades (Rob√¥ perde pontos)**  
```python
base_height = -10.0     # Por altura errada (muito alto/baixo)
orientation = -1.0      # Por estar inclinado demais
lin_vel_z = -2.0       # Por pular verticalmente
ang_vel_xy = -0.05     # Por rodar descontrolado
dof_vel = -1e-3        # Por mover juntas muito r√°pido
action_rate = -0.01    # Por mudar a√ß√µes bruscamente
```

**Como o rob√¥ aprende equil√≠brio:**
1. **Tenta movimento aleat√≥rio** ‚Üí Cai ‚Üí **Reward = -10** (altura) + **-1** (orienta√ß√£o) = **-11 pontos**
2. **Tenta ficar parado** ‚Üí Fica em p√© por 2 segundos ‚Üí **Reward = +0.15 √ó 100 steps = +15 pontos** ‚úÖ
3. **Policy aprende**: "Ficar em p√© = bom, cair = ruim"
4. **Repete milhares de vezes** at√© dominar o equil√≠brio

---

## üõ£Ô∏è **Se√ß√£o 3: Plano de A√ß√£o - 2 Estrat√©gias de Treinamento**

### üîß **Estrat√©gia 1: Treinamento Continuado (2-3 horas)**

**Objetivo**: Continuar treinamento do modelo atual at√© convergir

**Prepara√ß√£o**:
```bash
# Ativar ambiente e navegar para diret√≥rio
conda activate unitree-rl
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
cd ~/Workspaces/unitree_rl/isaacgym/python/examples/unitree_rl_gym
```

**Comando de Treinamento**:
```bash
# Resumir do checkpoint 10 atual
python legged_gym/scripts/train.py --task g1 --resume \
  --load_run Aug11_15-13-56_ \
  --checkpoint 10 \
  --max_iterations 3000
```

**Monitoramento TensorBoard**:
```bash
# Em terminal separado
tensorboard --logdir logs/g1/Aug11_15-13-56_
# Abrir: http://localhost:6006
```

**Checkpoints para testar durante treinamento**:
- **model_100.pt**: Teste b√°sico de estabilidade 
- **model_500.pt**: Primeiros movimentos coordenados  
- **model_1000.pt**: Equil√≠brio mais robusto
- **model_2000.pt**: Movimenta√ß√£o suave
- **model_3000.pt**: Performance final

**Teste de Checkpoints**:
```bash
# Testar checkpoint espec√≠fico durante treinamento
python legged_gym/scripts/play.py --task g1 \
  --load_run Aug11_15-13-56_ --checkpoint 500
```

---

### üèóÔ∏è **Estrat√©gia 2: Treino Completo do Zero (4-6 horas)**

**Objetivo**: Controle total sobre o processo de treinamento com configura√ß√£o otimizada

**Passo 1: Criar Configura√ß√£o Personalizada**

Criar arquivo `legged_gym/envs/g1/g1_teleop_config.py`:
```python
from legged_gym.envs.g1.g1_config import G1RoughCfg, G1RoughCfgPPO

class G1TeleopCfg(G1RoughCfg):
    """Configura√ß√£o otimizada para controle WASD teleoperado"""
    
    class env(G1RoughCfg.env):
        # Ambiente mais simples para foco em equil√≠brio
        num_envs = 4096  # Paraleliza√ß√£o m√°xima
        
    class terrain(G1RoughCfg.terrain):
        # Terrain simples para foco em standing/walking
        mesh_type = 'plane'
        num_rows = 1
        num_cols = 1
        
    class rewards(G1RoughCfg.rewards):
        # Enfatizar equil√≠brio e estabilidade para teleop
        class scales(G1RoughCfg.rewards.scales):
            # Rewards aumentados para estabilidade
            alive = 0.5           # 3x mais reward por ficar vivo
            contact = 0.25        # Mais reward por contato correto
            
            # Penalidades aumentadas para instabilidade  
            base_height = -20.0   # 2x penalidade por altura errada
            orientation = -5.0    # 5x penalidade por inclina√ß√£o
            lin_vel_z = -4.0      # 2x penalidade por pulos
            ang_vel_xy = -0.1     # 2x penalidade por rota√ß√µes indesejadas
            
            # Suavidade de movimento
            action_rate = -0.02   # 2x penalidade por mudan√ßas bruscas
            dof_vel = -2e-3       # 2x penalidade por movimentos r√°pidos
            
class G1TeleopCfgPPO(G1RoughCfgPPO):
    class runner(G1RoughCfgPPO.runner):
        experiment_name = 'g1_teleop'
        max_iterations = 5000
```

**Passo 2: Registrar Nova Task**

Adicionar ao `legged_gym/envs/__init__.py`:
```python
from legged_gym.envs.g1.g1_teleop_config import G1TeleopCfg, G1TeleopCfgPPO
# ... outras imports ...

task_registry.register("g1_teleop", G1, G1TeleopCfg(), G1TeleopCfgPPO())
```

**Passo 3: Iniciar Treinamento**
```bash
# Treino completo do zero
python legged_gym/scripts/train.py --task g1_teleop --max_iterations 5000
```

**Passo 4: Monitoramento**
```bash
# TensorBoard para nova task
tensorboard --logdir logs/g1_teleop/
# Abrir: http://localhost:6006
```

---

## üõ†Ô∏è **Se√ß√£o 4: Implementa√ß√µes T√©cnicas**

### üéØ **4.1 Standing Mode Natural via Reward Learning**

**Conceito**: O modelo aprende naturalmente standing mode atrav√©s do sistema de recompensas, **SEM necessidade de l√≥gica manual**.

**Como Funciona (Autom√°tico):**
```python
# Quando WASD n√£o pressionado: vx=0, wz=0
env.commands[:, 0] = 0.0    # Comando: "n√£o mover"
env.commands[:, 1] = 0.0    # Comando: "n√£o se deslocar lateralmente"  
env.commands[:, 2] = 0.0    # Comando: "n√£o girar"

# Modelo v√™ comando zero e aprende a:
# 1. Parar movimento (tracking_lin_vel reward)
# 2. Manter equil√≠brio (alive + orientation rewards)
# 3. Postura correta (base_height reward)
```

**Standing Mode = Comportamento Emergente:**
- ‚úÖ **Aprendido**: Modelo descobre como equilibrar parado
- ‚úÖ **Natural**: Transi√ß√µes suaves entre movimento ‚Üî parado
- ‚úÖ **Robusto**: Funciona mesmo com perturba√ß√µes externas
- ‚ùå **N√£o manual**: Sem posi√ß√µes articulares hard-coded

### ‚öôÔ∏è **4.2 Crit√©rios de Termina√ß√£o Relaxados**

**Problema**: Crit√©rios muito rigorosos fazem rob√¥ resetar facilmente.

**Valores Atuais (Muito Rigorosos)**:
```python
# legged_robot.py linha 122
self.reset_buf |= torch.abs(self.rpy[:,0]) > 0.8  # Roll > 46¬∞
self.reset_buf |= torch.abs(self.rpy[:,1]) > 1.0  # Pitch > 57¬∞
```

**Valores Propostos para Teleop (Mais Permissivos)**:
```python
# Tempor√°rio durante teleop - relaxar limites
if self.teleop_mode:  # Vari√°vel a ser criada
    self.reset_buf |= torch.abs(self.rpy[:,0]) > 1.2  # Roll > 69¬∞
    self.reset_buf |= torch.abs(self.rpy[:,1]) > 1.5  # Pitch > 86¬∞
else:
    # Manter rigoroso para treinamento
    self.reset_buf |= torch.abs(self.rpy[:,0]) > 0.8
    self.reset_buf |= torch.abs(self.rpy[:,1]) > 1.0
```

### üîÑ **4.3 Transi√ß√µes Suaves WASD ‚Üî Standing**

**Problema**: Mudan√ßas bruscas entre parado ‚Üî movimento causam instabilidade.

**Solu√ß√£o**: Interpola√ß√£o gradual
```python
class WASDController:
    def __init__(self):
        self.standing_alpha = 0.1  # Velocidade transi√ß√£o para standing
        self.walking_alpha = 0.2   # Velocidade transi√ß√£o para walking
        
    def smooth_transition(self, vx_target, wz_target):
        if vx_target == 0.0 and wz_target == 0.0:
            # Transi√ß√£o gradual para standing
            self.vx_cmd *= (1 - self.standing_alpha)
            self.wz_cmd *= (1 - self.standing_alpha)
        else:
            # Transi√ß√£o gradual para walking
            self.vx_cmd = self.vx_cmd * (1 - self.walking_alpha) + vx_target * self.walking_alpha
            self.wz_cmd = self.wz_cmd * (1 - self.walking_alpha) + wz_target * self.walking_alpha
```

---

## üìä **Se√ß√£o 5: Cronograma de Treinamento**

### **Etapa 1: Prepara√ß√£o (30 min)**
- [ ] Escolher estrat√©gia (continuado vs do zero)
- [ ] Ativar ambiente conda e configurar paths
- [ ] Inicializar TensorBoard para monitoramento
- [ ] Se estrat√©gia 2: Criar configura√ß√£o personalizada

### **Etapa 2: Treinamento Principal (2-6 horas)**
- [ ] Executar comando de treinamento escolhido
- [ ] Monitorar converg√™ncia via TensorBoard (rewards, episode length)
- [ ] Testar checkpoints intermedi√°rios com play.py
- [ ] Ajustar hiperpar√¢metros se necess√°rio

### **Etapa 3: Valida√ß√£o e Otimiza√ß√£o WASD (1 hora)**
- [ ] Testar melhor checkpoint com controles WASD
- [ ] Verificar estabilidade em standing mode
- [ ] Ajustar par√¢metros VX_BASE/WZ_BASE se necess√°rio
- [ ] Validar transi√ß√µes suaves parado ‚Üî movimento

### **Etapa 4: Documenta√ß√£o Final (30 min)**
- [ ] Documentar resultados e par√¢metros finais
- [ ] Atualizar arquivo de implementa√ß√£o WASD
- [ ] Commit e push das melhorias

---

## üîß **Se√ß√£o 6: Troubleshooting Comum**

### ‚ùå **"Treinamento n√£o converge"**
```python
# Sinais de problema:
1. Rewards oscilam muito ap√≥s 1000 iter
2. TensorBoard mostra plat√¥ baixo
3. Rob√¥ n√£o melhora comportamento

# Solu√ß√µes:
1. Reduzir learning rate
2. Ajustar rewards scales
3. Verificar configura√ß√£o do ambiente
```

---

## üéØ **Pr√≥ximos Passos**

Agora que voc√™ tem o guia completo, escolha a estrat√©gia de treinamento:

1. **üîß Treinamento Continuado** ‚Üí Continuar do modelo atual (2-3 horas)  
   *Pr√≥s: Aproveita progresso, mais r√°pido*  
   *Contras: Pode herdar problemas da configura√ß√£o inicial*

2. **üèóÔ∏è Treino Completo do Zero** ‚Üí Controle total com configura√ß√£o otimizada (4-6 horas)  
   *Pr√≥s: Configura√ß√£o limpa, rewards otimizados para teleop*  
   *Contras: Mais tempo, perde progresso anterior*

**Recomenda√ß√£o Baseada na Pesquisa**: 
- **Se tem tempo limitado**: Estrat√©gia 1 (continuado) - aproveita progresso
- **Se quer resultado cient√≠fico robusto**: Estrat√©gia 2 (do zero) - config otimizada para Multi-Task Learning
- **Ambas funcionam**: Modelo √∫nico aprender√° WASD + equil√≠brio integrados

---

**Criado por**: Claude Code  
**Data**: Agosto 2025  
**Status**: Guia completo para implementa√ß√£o